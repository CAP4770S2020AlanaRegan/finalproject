{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First obtain dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "Y4CJ2heZZHA2"
   },
   "outputs": [],
   "source": [
    "# Pyspark SQL\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import lit, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler, HashingTF, IDF, RegexTokenizer, StopWordsRemover, StringIndexer, IndexToString, Bucketizer, QuantileDiscretizer\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): langdetect in ./anaconda3/lib/python3.5/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): six in ./anaconda3/lib/python3.5/site-packages (from langdetect)\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package (langdetect) in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the following data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Login information\n",
    "# username = # AWS Username\n",
    "# password = # AWS Password\n",
    "# region = \"us-east-1\" # Change if different from your AWS region\n",
    "\n",
    "\n",
    "# Dataset location\n",
    "# s3 = #s3a address\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the AWS resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CluCfBnlZHA7"
   },
   "outputs": [],
   "source": [
    "sc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsAccessKeyId\", username)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsSecretAccessKey\", password)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.\" + region + \".amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test, df = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = df.rdd.map(tuple)\n",
    "# small = sc.parallelize(rdd.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes non-English data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def remove_nonenglish(row):\n",
    "    # Returns True if the tuple's description is written in English, false otherwise    \n",
    "    \n",
    "    try:\n",
    "        lang=detect(row[1])\n",
    "        if (lang == 'en'): \n",
    "            return True\n",
    "        else: \n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def replace_punc_with_space(desc):\n",
    "    # Returns an updated description string with punctuation directly between two letters replaced with a space\n",
    "    \n",
    "    new_desc=''\n",
    "    \n",
    "    for i in range(len(desc)-1):\n",
    "        new_desc+=desc[i]\n",
    "        if desc[i].islower() and desc[i+1].isupper():\n",
    "            new_desc+=' '\n",
    "    \n",
    "    new_desc+=desc[-1]\n",
    "    \n",
    "    return new_desc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genres converted to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genre_to_array(row):\n",
    "    genres = row[10]\n",
    "    glist = []\n",
    "    if(genres is not None): glist = genres.split('|')\n",
    "    if \"Nonfiction\" in glist: glist = 0.0\n",
    "    elif \"Fiction\" in glist: glist = 1.0\n",
    "    else: glist = None\n",
    "    \n",
    "    lst = list(row)\n",
    "    lst[10] = glist\n",
    "    tup = tuple(lst)\n",
    "\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_words(row):\n",
    "    # Returns tuple with description cleaned \n",
    "    # Removes punctuation, tokenizes words, stems them for comparision, filters out stop words\n",
    "    \n",
    "    desc = row[1] \n",
    "    \n",
    "    desc = replace_punc_with_space(desc) #Some words in descriptions are not separated by a space, but with punctuation\n",
    "    desc = desc.lower() #make all lowercase for easy comparing\n",
    "    \n",
    "    # split into words\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    words = word_tokenize(desc)\n",
    "    \n",
    "    # remove punctuation from each word\n",
    "    punc = str.maketrans('', '', string.punctuation)\n",
    "    no_punc = [word.translate(punc) for word in words]\n",
    "    \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words_alpha = [word for word in no_punc if word.isalpha()]\n",
    "    \n",
    "    \n",
    "    # filter out stop words\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words_alpha if not w in stop_words]    \n",
    "    \n",
    "    # stem the words\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in words]\n",
    "    \n",
    "    lst = list(row)\n",
    "    lst[1] = words\n",
    "    tup = tuple(lst)\n",
    "\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_null_genre(row):\n",
    "    if row[10] is None: return False\n",
    "    else: return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies the above processes to rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = rdd.filter(remove_nonenglish).map(clean_words).map(genre_to_array)\n",
    "rdd = rdd.filter(remove_null_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Kristin Hannah', ['alaska', 'unforgiving', 'untamedfor', 'family', 'crisis', 'ultimate', 'test', 'survivalernt', 'allbright', 'former', 'pow', 'comes', 'home', 'vietnam', 'war', 'changed', 'volatile', 'man', 'loses', 'yet', 'another', 'job', 'makes', 'impulsive', 'decision', 'move', 'family', 'north', 'alaska', 'live', 'grid', 'last', 'true', 'frontierthirteenyearold', 'leni', 'girl', 'coming', 'age', 'tumultuous', 'time', 'caught', 'riptide', 'passionate', 'stormy', 'relationship', 'dares', 'hope', 'new', 'land', 'lead', 'better', 'future', 'family', 'desperate', 'place', 'belong', 'mother', 'cora', 'anything', 'go', 'anywhere', 'man', 'loves', 'even', 'means', 'following', 'unknown', 'first', 'alaska', 'seems', 'answer', 'prayers', 'wild', 'remote', 'corner', 'state', 'find', 'fiercely', 'independent', 'community', 'strong', 'men', 'even', 'stronger', 'women', 'long', 'sunlit', 'days', 'generosity', 'locals', 'make', 'lack', 'preparation', 'dwindling', 'resourcesbut', 'winter', 'approaches', 'darkness', 'descends', 'alaska', 'fragile', 'mental', 'state', 'deteriorates', 'family', 'begins', 'fracture', 'soon', 'perils', 'outside', 'pale', 'comparison', 'threats', 'within', 'small', 'cabin', 'covered', 'snow', 'blanketed', 'eighteen', 'hours', 'night', 'leni', 'mother', 'learn', 'terrible', 'truth', 'wild', 'one', 'save', 'themselvesin', 'unforgettable', 'portrait', 'human', 'frailty', 'resilience', 'kristin', 'hannah', 'reveals', 'indomitable', 'character', 'modern', 'american', 'pioneer', 'spirit', 'vanishing', 'place', 'incomparable', 'beauty', 'danger', 'great', 'alone', 'daring', 'beautiful', 'stayupallnight', 'story', 'love', 'loss', 'fight', 'survival', 'wildness', 'lives', 'man', 'nature'], None, 'Kindle Edition', None, '435 pages', '4.33', '146505', '17438', 'The Great Alone', 1.0, 'https://images.gr-assets.com/books/1501852423l/34912895.jpg')]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data to dataframe with header names and cast datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- description: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rating: decimal(3,2) (nullable = true)\n",
      " |-- ratingCount: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "booksdf = rdd.toDF(['author', 'description', 'edition', 'format', \n",
    "                    'isbn13', 'pages', 'rating', 'ratingCount', \n",
    "                    'review_count', 'title', 'genres', 'image_url']) \\\n",
    "    .drop(\"edition\") \\\n",
    "    .drop(\"format\") \\\n",
    "    .drop(\"pages\") \\\n",
    "    .drop(\"isbn13\") \\\n",
    "    .drop(\"review_count\") \\\n",
    "    .drop(\"image_url\")\n",
    "\n",
    "booksdf = booksdf.withColumn(\"rating\", booksdf[\"rating\"].cast(\"decimal(3,2)\")) \\\n",
    "                .withColumn(\"ratingCount\", booksdf[\"ratingCount\"].cast(\"long\"))\n",
    "booksdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all elements with a null description or rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+\n",
      "|               title|rating|genres|\n",
      "+--------------------+------+------+\n",
      "|     The Great Alone|  4.33|   1.0|\n",
      "|               Circe|  4.34|   1.0|\n",
      "|The Woman in the ...|  3.97|   1.0|\n",
      "|Children of Blood...|  4.25|   1.0|\n",
      "|An American Marriage|  4.01|   1.0|\n",
      "| The Wife Between Us|  3.85|   1.0|\n",
      "|    The Immortalists|  3.73|   1.0|\n",
      "|        The Outsider|  4.07|   1.0|\n",
      "|   The Kiss Quotient|  4.00|   1.0|\n",
      "|Where the Crawdad...|  4.52|   1.0|\n",
      "+--------------------+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "booksdf = booksdf.dropna(subset=('description', 'rating', 'genres'))\n",
    "booksdf[(\"title\", \"rating\", \"genres\")].show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- description: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rating: decimal(3,2) (nullable = true)\n",
      " |-- ratingCount: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "booksdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------+\n",
      "|         description|rating|ratingCount|\n",
      "+--------------------+------+-----------+\n",
      "|[alaska, unforgiv...|  4.33|     146505|\n",
      "|[house, helios, g...|  4.34|      61357|\n",
      "|[anna, fox, lives...|  3.97|     175678|\n",
      "|[killed, motherth...|  4.25|      56789|\n",
      "|[newlyweds, celes...|  4.01|      91515|\n",
      "|[read, book, make...|  3.85|      89835|\n",
      "|[knew, date, deat...|  3.73|      61815|\n",
      "|[unspeakable, cri...|  4.07|      65894|\n",
      "|[heartwarming, re...|  4.00|      33760|\n",
      "|[years, rumors, h...|  4.52|      40414|\n",
      "+--------------------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = RegexTokenizer(inputCol=\"description\", outputCol=\"descToken\", pattern=\"\\\\W\")\n",
    "#booksdf = tokenizer.transform(booksdf)\n",
    "\n",
    "#swremover = StopWordsRemover(inputCol=\"descToken\", outputCol=\"desc\")\n",
    "#booksdf = swremover.transform(booksdf)\n",
    "\n",
    "booksdf[(\"description\",\"rating\",\"ratingCount\")].show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.QuantileDiscretizer\n",
    "\n",
    "One method is with **QuantileDiscretizer**, which buckets by frequency (equal number in each bucket).\n",
    "\n",
    "Adjust **numBuckets** to change the bucket sizes for the ratings and get different outcomes. The lower number of buckets, the more book ratings per bucket.\n",
    "\n",
    "Sample code:\n",
    "\n",
    "```\n",
    "discretizer = QuantileDiscretizer(numBuckets=16, inputCol='rating', outputCol='label')\n",
    "booksdf = discretizer.fit(booksdf).transform(booksdf)\n",
    "```\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.Bucketizer\n",
    "\n",
    "Alternative is to use **Bucketizer**, which buckets by length (or range). Adjust the numbers in the range to fix the outcome.\n",
    "\n",
    "Sample code:\n",
    "\n",
    "```\n",
    "splitWhole = [0,1,2,3,4,5]\n",
    "splitQuarter = [0,0.25,0.5,0.75,1,1.25,1.5,1.75,2,2.25,2.5,2.75,3,3.25,3.5,3.75,4,4.25,4.5,4.75,5]\n",
    "splitHalf = [0,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5]\n",
    "bucketizer = Bucketizer(splits=split, inputCol='rating', outputCol='label')\n",
    "booksdf = bucketizer.transform(booksdf)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split = [0,1,2,3,4,5]\n",
    "bucketizer = Bucketizer(splits=split, inputCol='rating', outputCol='label')\n",
    "booksdf = bucketizer.transform(booksdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- description: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rating: decimal(3,2) (nullable = true)\n",
      " |-- ratingCount: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "booksdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get weights for ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the number of ratings into weight for the ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"ratingCount\"], outputCol=\"countVec\")\n",
    "scaler = MinMaxScaler(inputCol='countVec', outputCol='weightVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "booksdf = pipeline.fit(booksdf).transform(booksdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ith_(v, i):\n",
    "    try:\n",
    "        return float(v[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "ith = udf(ith_, DoubleType())\n",
    "booksdf = booksdf.withColumn(\"weight\", ith(\"weightVec\", lit(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+-------------------+\n",
      "|               title|rating|label|             weight|\n",
      "+--------------------+------+-----+-------------------+\n",
      "|     The Great Alone|  4.33|  4.0|  0.833929161021484|\n",
      "|               Circe|  4.34|  4.0|0.34921384900891467|\n",
      "|The Woman in the ...|  3.97|  3.0|                1.0|\n",
      "|Children of Blood...|  4.25|  4.0|0.32320995525599716|\n",
      "|An American Marriage|  4.01|  4.0| 0.5208919198934341|\n",
      "| The Wife Between Us|  3.85|  3.0| 0.5113283162364942|\n",
      "|    The Immortalists|  3.73|  3.0| 0.3518210695296756|\n",
      "|        The Outsider|  4.07|  4.0|0.37504127150387667|\n",
      "|   The Kiss Quotient|  4.00|  4.0| 0.1921145810800041|\n",
      "|Where the Crawdad...|  4.52|  4.0|0.22999328270695524|\n",
      "+--------------------+------+-----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "booksdf[(\"title\", \"rating\", \"label\", \"weight\")].show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"genre\", outputCol=\"genreLabel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "booksdf = indexer.fit(booksdf).transform(booksdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|               title|              genres|genreLabel|\n",
      "+--------------------+--------------------+----------+\n",
      "|     The Great Alone|[Fiction, Histori...|       1.0|\n",
      "|               Circe|[Fantasy, Fiction...|       2.0|\n",
      "|    The Cruel Prince|[Fantasy, Young A...|       2.0|\n",
      "|The Woman in the ...|[Mystery, Thrille...|       3.0|\n",
      "|Children of Blood...|[Fantasy, Young A...|       2.0|\n",
      "|An American Marriage|[Fiction, Contemp...|       1.0|\n",
      "| The Wife Between Us|[Thriller, Fictio...|       9.0|\n",
      "|         Thunderhead|[Young Adult, Sci...|       5.0|\n",
      "|A Court of Frost ...|[Fantasy, New Adu...|       2.0|\n",
      "|    The Immortalists|[Fiction, Fantasy...|       1.0|\n",
      "+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "booksdf = booksdf.dropna(subset=('genre', 'genreLabel'))\n",
    "booksdf[(\"title\", \"genres\", \"genreLabel\")].show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYdGn8tsZHBQ"
   },
   "source": [
    "### TF/IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF\n",
    "\n",
    "**numFeatures should be adjusted to get better outcomes**\n",
    "\n",
    "> Since a simple modulo is used to transform the hash function to a column index, it is advisable to **use a power of two** as the numFeatures parameter; otherwise the features will not be mapped evenly to the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"description\", outputCol=\"rawFeatures\", numFeatures=32)\n",
    "featurizedData = hashingTF.transform(booksdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         description|         rawFeatures|\n",
      "+--------------------+--------------------+\n",
      "|[alaska, unforgiv...|(32,[0,1,2,3,4,5,...|\n",
      "|[house, helios, g...|(32,[0,1,2,3,4,5,...|\n",
      "|[anna, fox, lives...|(32,[1,2,3,5,6,7,...|\n",
      "|[killed, motherth...|(32,[0,1,2,4,5,6,...|\n",
      "|[newlyweds, celes...|(32,[1,2,4,5,6,7,...|\n",
      "|[read, book, make...|(32,[0,1,2,3,4,5,...|\n",
      "|[knew, date, deat...|(32,[1,2,3,4,5,6,...|\n",
      "|[unspeakable, cri...|(32,[0,1,2,3,5,6,...|\n",
      "|[heartwarming, re...|(32,[0,1,2,3,4,5,...|\n",
      "|[years, rumors, h...|(32,[1,2,3,4,5,8,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurizedData.select('description', 'rawFeatures').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+\n",
      "|rating|label|         description|            features|\n",
      "+------+-----+--------------------+--------------------+\n",
      "|  4.33|  4.0|[alaska, unforgiv...|(32,[0,1,2,3,4,5,...|\n",
      "|  4.34|  4.0|[house, helios, g...|(32,[0,1,2,3,4,5,...|\n",
      "|  3.97|  3.0|[anna, fox, lives...|(32,[1,2,3,5,6,7,...|\n",
      "|  4.25|  4.0|[killed, motherth...|(32,[0,1,2,4,5,6,...|\n",
      "|  4.01|  4.0|[newlyweds, celes...|(32,[1,2,4,5,6,7,...|\n",
      "|  3.85|  3.0|[read, book, make...|(32,[0,1,2,3,4,5,...|\n",
      "|  3.73|  3.0|[knew, date, deat...|(32,[1,2,3,4,5,6,...|\n",
      "|  4.07|  4.0|[unspeakable, cri...|(32,[0,1,2,3,5,6,...|\n",
      "|  4.00|  4.0|[heartwarming, re...|(32,[0,1,2,3,4,5,...|\n",
      "|  4.52|  4.0|[years, rumors, h...|(32,[1,2,3,4,5,8,...|\n",
      "+------+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaledData.select(\"rating\", \"label\", \"description\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = rescaledData.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descision Tree Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.classification.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------+-----+\n",
      "|               title|         description|            features|rating|label|\n",
      "+--------------------+--------------------+--------------------+------+-----+\n",
      "|      Dear Mrs. Bird|[listening, lengt...|(32,[0,2,3,4,5,6,...|  3.81|  3.0|\n",
      "|Ask Me About My U...|[woman, experienc...|(32,[0,1,2,3,5,6,...|  3.95|  3.0|\n",
      "|The Enchanted Gar...|[sixth, birthday,...|(32,[0,1,3,4,5,6,...|  4.34|  4.0|\n",
      "|         Dogs of War|[name, rex, good,...|(32,[0,2,3,4,5,6,...|  4.33|  4.0|\n",
      "|     Sky in the Deep|[part, wonder, wo...|(32,[0,1,2,4,5,6,...|  4.05|  4.0|\n",
      "|  The Perfect Mother|[vanity, fair, ca...|(32,[0,1,2,3,4,5,...|  3.55|  3.0|\n",
      "|Daughters of the ...|[celebrated, regi...|(32,[0,1,2,3,4,5,...|  4.19|  4.0|\n",
      "|            The Wife|[scandal, secretw...|(32,[0,1,2,3,4,5,...|  3.93|  3.0|\n",
      "|The Sparsholt Affair|[internationally,...|(32,[0,1,2,3,4,5,...|  3.59|  3.0|\n",
      "|      The Book Ninja|[sometimes, love,...|(32,[0,1,2,3,5,6,...|  3.84|  3.0|\n",
      "+--------------------+--------------------+--------------------+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.select('title', 'description', 'features', 'rating', 'label').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", cacheNodeIds = True, checkpointInterval = 10, impurity = 'gini')\n",
    "dtModel = dt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+--------------------+--------------------+\n",
      "|prediction|rating|label|         description|            features|\n",
      "+----------+------+-----+--------------------+--------------------+\n",
      "|       3.0|  4.51|  4.0|[author, new, yor...|(32,[0,3,5,7,8,9,...|\n",
      "|       3.0|  3.97|  3.0|[anna, fox, lives...|(32,[1,2,3,5,6,7,...|\n",
      "|       3.0|  3.99|  3.0|[flavia, enjoying...|(32,[1,2,3,5,6,7,...|\n",
      "|       4.0|  3.98|  3.0|[bestselling, aut...|(32,[0,1,2,3,4,5,...|\n",
      "|       3.0|  3.77|  3.0|[stunning, new, p...|(32,[0,1,2,3,5,6,...|\n",
      "|       4.0|  4.13|  4.0|[acclaimed, autho...|(32,[0,1,2,3,4,5,...|\n",
      "|       4.0|  3.90|  3.0|[new, york, times...|(32,[0,1,2,3,4,5,...|\n",
      "|       4.0|  3.99|  3.0|[blazingly, origi...|(32,[0,1,2,4,5,6,...|\n",
      "|       4.0|  3.91|  3.0|[smart, funny, ne...|(32,[0,1,2,3,4,5,...|\n",
      "|       3.0|  3.40|  3.0|[betts, meets, ai...|(32,[1,2,3,4,5,7,...|\n",
      "+----------+------+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtPredictions = dtModel.transform(testData)\n",
    "dtPredictions.select(\"prediction\", \"rating\",\"label\",\"description\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5490196078431373\n"
     ]
    }
   ],
   "source": [
    "dtAccuracy = evaluator.evaluate(dtPredictions)\n",
    "print(dtAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       3.0|  4.0|(32,[0,3,5,7,8,9,...|\n",
      "|       3.0|  3.0|(32,[1,2,3,5,6,7,...|\n",
      "|       3.0|  3.0|(32,[1,2,3,5,6,7,...|\n",
      "|       4.0|  3.0|(32,[0,1,2,3,4,5,...|\n",
      "|       3.0|  3.0|(32,[0,1,2,3,5,6,...|\n",
      "|       4.0|  4.0|(32,[0,1,2,3,4,5,...|\n",
      "|       4.0|  3.0|(32,[0,1,2,3,4,5,...|\n",
      "|       4.0|  3.0|(32,[0,1,2,4,5,6,...|\n",
      "|       4.0|  3.0|(32,[0,1,2,3,4,5,...|\n",
      "|       3.0|  3.0|(32,[1,2,3,4,5,7,...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtPredictions.select(\"prediction\", \"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"label\", \\\n",
    "                        featuresCol=\"features\", \\\n",
    "                        maxIter=20, \\\n",
    "                        regParam=0.3, \\\n",
    "                        elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       3.0|  4.0|(32,[0,3,5,7,8,9,...|\n",
      "|       3.0|  3.0|(32,[1,2,3,5,6,7,...|\n",
      "|       4.0|  3.0|(32,[0,1,2,3,5,6,...|\n",
      "|       4.0|  4.0|(32,[0,2,3,4,5,6,...|\n",
      "|       3.0|  3.0|(32,[0,1,2,3,4,5,...|\n",
      "|       3.0|  3.0|(32,[0,1,2,3,4,5,...|\n",
      "|       4.0|  3.0|(32,[0,1,2,3,4,5,...|\n",
      "|       3.0|  3.0|(32,[0,1,2,3,4,5,...|\n",
      "|       3.0|  3.0|(32,[0,1,2,3,4,5,...|\n",
      "|       3.0|  3.0|(32,[0,1,2,3,4,5,...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrPredictions = lrModel.transform(testData)\n",
    "lrPredictions.select(\"prediction\", \"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "lrAccuracy = evaluator.evaluate(lrPredictions)\n",
    "print(lrAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "             .build())\n",
    "\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "\n",
    "cvModel = cv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvPredictions = cvModel.transform(testData)\n",
    "cvPredictions.select(\"prediction\", \"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvAccuracy = evaluator.evaluate(cvPredictions)\n",
    "print(cvAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.classification.RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees=100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "rfModel = rf.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfPredictions = rfModel.transform(testData)\n",
    "rfPredictions.select(\"prediction\", \"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfAccuracy = evaluator.evaluate(rfPredictions)\n",
    "print(rfAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Multinomial Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.classification.NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- rating: decimal(3,2) (nullable = true)\n",
      " |-- ratingCount: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- countVec: vector (nullable = true)\n",
      " |-- weightVec: vector (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- descToken: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- desc: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rawFeatures: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes(labelCol=\"label\", featuresCol=\"features\", modelType=\"multinomial\")\n",
    "nbModel = nb.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       2.0|  4.0|(32,[0,1,2,3,4,5,...|\n",
      "|       2.0|  3.0|(32,[0,1,2,3,4,5,...|\n",
      "|       2.0|  3.0|(32,[0,1,2,3,4,5,...|\n",
      "|       2.0|  4.0|(32,[0,1,2,3,4,5,...|\n",
      "|       2.0|  3.0|(32,[0,2,3,4,5,6,...|\n",
      "|       2.0|  4.0|(32,[0,1,2,3,5,6,...|\n",
      "|       2.0|  3.0|(32,[0,1,2,3,5,6,...|\n",
      "|       2.0|  3.0|(32,[0,1,2,3,4,5,...|\n",
      "|       2.0|  4.0|(32,[0,1,2,3,4,5,...|\n",
      "|       2.0|  3.0|(32,[0,1,2,4,5,6,...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nbPredictions = nbModel.transform(testData)\n",
    "nbPredictions.select(\"prediction\", \"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "nbAccuracy = evaluator.evaluate(nbPredictions)\n",
    "print(nbAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes(labelCol=\"label\", featuresCol=\"features\", modelType=\"multinomial\", weightCol=\"weight\")\n",
    "nbModel = nb.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbPredictions = nbModel.transform(testData)\n",
    "nbPredictions.select(\"prediction\", \"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbAccuracy = evaluator.evaluate(nbPredictions)\n",
    "print(nbAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "FinalProject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
