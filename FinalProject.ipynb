{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First obtain dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "Y4CJ2heZZHA2"
   },
   "outputs": [],
   "source": [
    "# Pyspark SQL\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import lit, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler, HashingTF, IDF, RegexTokenizer, StopWordsRemover, Bucketizer, QuantileDiscretizer\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): langdetect in ./anaconda3/lib/python3.5/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): six in ./anaconda3/lib/python3.5/site-packages (from langdetect)\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package (langdetect) in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the following data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Login information\n",
    "# username = # AWS Username\n",
    "# password = # AWS Password\n",
    "# region = \"us-east-1\" # Change if different from your AWS region\n",
    "\n",
    "\n",
    "# Dataset location\n",
    "# s3 = #s3a address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the AWS resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CluCfBnlZHA7"
   },
   "outputs": [],
   "source": [
    "sc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsAccessKeyId\", username)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsSecretAccessKey\", password)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.\" + region + \".amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = df.rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# small = sc.parallelize(rdd.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes non-English data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def remove_nonenglish(row):\n",
    "    '''\n",
    "    Removes records that have invalid descriptions from the dataframe\n",
    "    Input: dataframe\n",
    "    Output: Cleaned up dataframe\n",
    "    '''\n",
    "    try:\n",
    "        lang=detect(row[1])\n",
    "        if (lang == 'en'): \n",
    "            return True\n",
    "        else: \n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def replace_punc_with_space(desc):\n",
    "    \n",
    "    upd_desc=''\n",
    "    \n",
    "    for i in range(len(desc)-1):\n",
    "        upd_desc+=desc[i]\n",
    "        if desc[i].islower() and desc[i+1].isupper():\n",
    "            upd_desc+=' '\n",
    "    \n",
    "    upd_desc+=desc[-1]\n",
    "    return upd_desc \n",
    "\n",
    "def remove_punc(row):\n",
    "    desc = row[1]\n",
    "    \n",
    "    desc=replace_punc_with_space(desc)    \n",
    "    desc=desc.lower() \n",
    "    desc = \"\".join([\" \" if char in ['.', ',', '?', '!', '(', ')', '/', ';', ':'] else char for char in desc])\n",
    "    desc = \"\".join([\"\" if char in ['\\''] else char for char in desc])\n",
    "    \n",
    "    lst = list(row)\n",
    "    lst[1] = desc\n",
    "    tup = tuple(lst)\n",
    "\n",
    "    return tup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# small = small.filter(remove_nonenglish).map(remove_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genres converted to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genre_to_array(row):\n",
    "    genres = row[10]\n",
    "    glist = []\n",
    "    \n",
    "    if(genres is not None): glist = genres.split('|')\n",
    "    \n",
    "    lst = list(row)\n",
    "    lst[10] = glist\n",
    "    tup = tuple(lst)\n",
    "\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies the above processes to rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = rdd.filter(remove_nonenglish).map(remove_punc).map(genre_to_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Kristin Hannah', 'alaska  1974 unpredictable  unforgiving  untamed for a family in crisis  the ultimate test of survival ernt allbright  a former pow  comes home from the vietnam war a changed and volatile man  when he loses yet another job  he makes an impulsive decision  he will move his family north  to alaska  where they will live off the grid in america’s last true frontier thirteen-year-old leni  a girl coming of age in a tumultuous time  caught in the riptide of her parents’ passionate  stormy relationship  dares to hope that a new land will lead to a better future for her family  she is desperate for a place to belong  her mother  cora  will do anything and go anywhere for the man she loves  even if it means following him into the unknown at first  alaska seems to be the answer to their prayers  in a wild  remote corner of the state  they find a fiercely independent community of strong men and even stronger women  the long  sunlit days and the generosity of the locals make up for the allbrights’ lack of preparation and dwindling resources but as winter approaches and darkness descends on alaska  ernt’s fragile mental state deteriorates and the family begins to fracture  soon the perils outside pale in comparison to threats from within  in their small cabin  covered in snow  blanketed in eighteen hours of night  leni and her mother learn the terrible truth  they are on their own  in the wild  there is no one to save them but themselves in this unforgettable portrait of human frailty and resilience  kristin hannah reveals the indomitable character of the modern american pioneer and the spirit of a vanishing alaska―a place of incomparable beauty and danger  the great alone is a daring  beautiful  stay-up-all-night story about love and loss  the fight for survival  and the wildness that lives in both man and nature ', None, 'Kindle Edition', None, '435 pages', '4.33', '146505', '17438', 'The Great Alone', ['Fiction', 'Historical', 'Historical Fiction', 'Audiobook', 'Historical', 'Adult Fiction', 'Adult', 'Family', 'Romance', 'Adventure', 'Contemporary'], 'https://images.gr-assets.com/books/1501852423l/34912895.jpg')]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data to dataframe with header names and cast datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- rating: decimal(3,2) (nullable = true)\n",
      " |-- ratingCount: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "booksdf = rdd.toDF(['author', 'description', 'edition', 'format', \n",
    "                    'isbn13', 'pages', 'rating', 'ratingCount', \n",
    "                    'review_count', 'title', 'genres', 'image_url']) \\\n",
    "    .drop(\"edition\") \\\n",
    "    .drop(\"format\") \\\n",
    "    .drop(\"pages\") \\\n",
    "    .drop(\"isbn13\") \\\n",
    "    .drop(\"review_count\") \\\n",
    "    .drop(\"image_url\")\n",
    "\n",
    "booksdf = booksdf.withColumn(\"rating\", booksdf[\"rating\"].cast(\"decimal(3,2)\")) \\\n",
    "                .withColumn(\"ratingCount\", booksdf[\"ratingCount\"].cast(\"long\"))\n",
    "booksdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all elements with a null description or rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "booksdf = booksdf.dropna(subset=('description', 'rating'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.QuantileDiscretizer\n",
    "\n",
    "One method is with **QuantileDiscretizer**, which buckets by frequency (equal number in each bucket).\n",
    "\n",
    "Adjust **numBuckets** to change the bucket sizes for the ratings and get different outcomes. The lower number of buckets, the more book ratings per bucket.\n",
    "\n",
    "Sample code:\n",
    "\n",
    "```\n",
    "discretizer = QuantileDiscretizer(numBuckets=16, inputCol='rating', outputCol='label')\n",
    "booksdf = discretizer.fit(booksdf).transform(booksdf)\n",
    "```\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.Bucketizer\n",
    "\n",
    "Alternative is to use **Bucketizer**, which buckets by length (or range). Adjust the numbers in the range to fix the outcome.\n",
    "\n",
    "Sample code:\n",
    "\n",
    "```\n",
    "splitQuarter = [0,0.25,0.5,0.75,1,1.25,1.5,1.75,2,2.25,2.5,2.75,3,3.25,3.5,3.75,4,4.25,4.5,4.75,5]\n",
    "splitHalf = [0,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5]\n",
    "bucketizer = Bucketizer(splits=split, inputCol='rating', outputCol='label')\n",
    "booksdf = bucketizer.transform(booksdf)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split = [0,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5]\n",
    "bucketizer = Bucketizer(splits=split, inputCol='rating', outputCol='label')\n",
    "booksdf = bucketizer.transform(booksdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the number of ratings into weight for Naive Bayes classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"ratingCount\"], outputCol=\"countVec\")\n",
    "scaler = MinMaxScaler(inputCol='countVec', outputCol='weightVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "booksdf = pipeline.fit(booksdf).transform(booksdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ith_(v, i):\n",
    "    try:\n",
    "        return float(v[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "ith = udf(ith_, DoubleType())\n",
    "booksdf = booksdf.withColumn(\"weight\", ith(\"weightVec\", lit(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+\n",
      "|rating|label|             weight|\n",
      "+------+-----+-------------------+\n",
      "|  4.33|  8.0| 0.8339395595325512|\n",
      "|  4.34|  8.0| 0.3492545979268772|\n",
      "|  4.18|  8.0|0.35802068569021556|\n",
      "|  3.97|  7.0|                1.0|\n",
      "|  4.25|  8.0|0.32325233240549417|\n",
      "|  4.01|  8.0| 0.5209219192039937|\n",
      "|  3.85|  7.0| 0.5113589143712609|\n",
      "|  4.53|  9.0|0.12990886684084996|\n",
      "|  3.92|  7.0|  0.369775212463783|\n",
      "|  3.73|  7.0| 0.3518616551967531|\n",
      "+------+-----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "booksdf[(\"rating\", \"label\", \"weight\")].show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYdGn8tsZHBQ"
   },
   "source": [
    "### TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         description|           descToken|                desc|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|alaska  1974 unpr...|[alaska, 1974, un...|[alaska, 1974, un...|\n",
      "|in the house of h...|[in, the, house, ...|[house, helios, g...|\n",
      "|of course i want ...|[of, course, i, w...|[course, want, li...|\n",
      "|anna fox lives al...|[anna, fox, lives...|[anna, fox, lives...|\n",
      "|they killed my mo...|[they, killed, my...|[killed, mother, ...|\n",
      "|newlyweds celesti...|[newlyweds, celes...|[newlyweds, celes...|\n",
      "|when you read thi...|[when, you, read,...|[read, book, make...|\n",
      "|rowan has gone ro...|[rowan, has, gone...|[rowan, gone, rog...|\n",
      "|hope warms the co...|[hope, warms, the...|[hope, warms, col...|\n",
      "|if you knew the d...|[if, you, knew, t...|[knew, date, deat...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer(inputCol=\"description\", outputCol=\"descToken\", pattern=\"\\\\W\")\n",
    "booksdf = tokenizer.transform(booksdf)\n",
    "\n",
    "swremover = StopWordsRemover(inputCol=\"descToken\", outputCol=\"desc\")\n",
    "booksdf = swremover.transform(booksdf)\n",
    "\n",
    "booksdf[(\"description\", \"descToken\", \"desc\")].show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF\n",
    "\n",
    "**numFeatures should be adjusted to get better outcomes**\n",
    "\n",
    "> Since a simple modulo is used to transform the hash function to a column index, it is advisable to **use a power of two** as the numFeatures parameter; otherwise the features will not be mapped evenly to the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"desc\", outputCol=\"rawFeatures\", numFeatures=32)\n",
    "featurizedData = hashingTF.transform(booksdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                desc|         rawFeatures|\n",
      "+--------------------+--------------------+\n",
      "|[alaska, 1974, un...|(32,[0,1,2,3,4,5,...|\n",
      "|[house, helios, g...|(32,[0,1,2,3,4,5,...|\n",
      "|[course, want, li...|(32,[0,1,2,3,4,5,...|\n",
      "|[anna, fox, lives...|(32,[1,2,3,5,6,7,...|\n",
      "|[killed, mother, ...|(32,[0,1,2,4,5,6,...|\n",
      "|[newlyweds, celes...|(32,[1,2,4,5,6,7,...|\n",
      "|[read, book, make...|(32,[0,1,2,3,5,6,...|\n",
      "|[rowan, gone, rog...|(32,[0,1,2,3,4,5,...|\n",
      "|[hope, warms, col...|(32,[0,2,3,5,6,7,...|\n",
      "|[knew, date, deat...|(32,[1,2,3,4,5,6,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurizedData.select('desc', 'rawFeatures').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|               title|label|            features|\n",
      "+--------------------+-----+--------------------+\n",
      "|     The Great Alone|  8.0|(32,[0,1,2,3,4,5,...|\n",
      "|               Circe|  8.0|(32,[0,1,2,3,4,5,...|\n",
      "|    The Cruel Prince|  8.0|(32,[0,1,2,3,4,5,...|\n",
      "|The Woman in the ...|  7.0|(32,[1,2,3,5,6,7,...|\n",
      "|Children of Blood...|  8.0|(32,[0,1,2,4,5,6,...|\n",
      "|An American Marriage|  8.0|(32,[1,2,4,5,6,7,...|\n",
      "| The Wife Between Us|  7.0|(32,[0,1,2,3,5,6,...|\n",
      "|         Thunderhead|  9.0|(32,[0,1,2,3,4,5,...|\n",
      "|A Court of Frost ...|  7.0|(32,[0,2,3,5,6,7,...|\n",
      "|    The Immortalists|  7.0|(32,[1,2,3,4,5,6,...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaledData.select(\"title\", \"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = rescaledData.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descision Tree Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.classification.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", cacheNodeIds = True, checkpointInterval = 10, impurity = 'entropy')\n",
    "dtModel = dt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       8.0|  9.0|(32,[3,5,7,8,9,11...|\n",
      "|       8.0|  8.0|(32,[1,2,3,4,5,6,...|\n",
      "|       7.0|  8.0|(32,[0,1,2,3,4,5,...|\n",
      "|       8.0|  7.0|(32,[0,1,2,3,4,5,...|\n",
      "|       8.0|  7.0|(32,[0,1,2,3,5,6,...|\n",
      "|       8.0|  8.0|(32,[0,1,2,3,4,5,...|\n",
      "|       7.0|  8.0|(32,[0,1,2,3,4,5,...|\n",
      "|       8.0|  7.0|(32,[0,2,3,5,6,7,...|\n",
      "|       7.0|  8.0|(32,[0,1,2,3,4,5,...|\n",
      "|       8.0|  8.0|(32,[0,3,4,5,6,7,...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtPredictions = dtModel.transform(testData)\n",
    "dtPredictions.select(\"prediction\", \"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47126436781609193\n"
     ]
    }
   ],
   "source": [
    "dtAccuracy = evaluator.evaluate(dtPredictions)\n",
    "print(dtAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       8.0|  9.0|(32,[3,5,7,8,9,11...|\n",
      "|       7.0|  8.0|(32,[1,2,3,4,5,6,...|\n",
      "|       8.0|  8.0|(32,[0,1,2,3,4,5,...|\n",
      "|       8.0|  7.0|(32,[0,1,2,3,4,5,...|\n",
      "|       8.0|  7.0|(32,[0,1,2,3,5,6,...|\n",
      "|       8.0|  8.0|(32,[0,1,2,3,4,5,...|\n",
      "|       8.0|  8.0|(32,[0,1,2,3,4,5,...|\n",
      "|       7.0|  7.0|(32,[0,2,3,5,6,7,...|\n",
      "|       7.0|  8.0|(32,[0,1,2,3,4,5,...|\n",
      "|       8.0|  8.0|(32,[0,3,4,5,6,7,...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtPredictions = dtModel.transform(testData)\n",
    "dtPredictions.select(\"prediction\", \"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46360153256704983\n"
     ]
    }
   ],
   "source": [
    "dtAccuracy = evaluator.evaluate(dtPredictions)\n",
    "print(dtAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.classification.RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "rfModel = rf.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       7.0|  9.0|(32,[3,5,7,8,9,11...|\n",
      "|       8.0|  8.0|(32,[1,2,3,4,5,6,...|\n",
      "|       8.0|  8.0|(32,[0,1,2,3,4,5,...|\n",
      "|       7.0|  7.0|(32,[0,1,2,3,4,5,...|\n",
      "|       8.0|  7.0|(32,[0,1,2,3,5,6,...|\n",
      "|       8.0|  8.0|(32,[0,1,2,3,4,5,...|\n",
      "|       8.0|  8.0|(32,[0,1,2,3,4,5,...|\n",
      "|       8.0|  7.0|(32,[0,2,3,5,6,7,...|\n",
      "|       8.0|  8.0|(32,[0,1,2,3,4,5,...|\n",
      "|       7.0|  8.0|(32,[0,3,4,5,6,7,...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfPredictions = rfModel.transform(testData)\n",
    "rfPredictions.select(\"prediction\", \"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5325670498084292\n"
     ]
    }
   ],
   "source": [
    "rfAccuracy = evaluator.evaluate(rfPredictions)\n",
    "print(rfAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Multinomial Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.classification.NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes(labelCol=\"label\", featuresCol=\"features\", modelType=\"bernoulli\", weightCol=\"weight\")\n",
    "nbModel = nb.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbPredictions = nbModel.transform(testData)\n",
    "nbPredictions.select(\"prediction\", \"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbAccuracy = evaluator.evaluate(nbPredictions)\n",
    "print(nbAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes(labelCol=\"label\", featuresCol=\"features\", modelType=\"multinomial\", weightCol=\"weight\")\n",
    "nbModel = nb.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbPredictions = nbModel.transform(testData)\n",
    "nbPredictions.select(\"prediction\", \"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbAccuracy = evaluator.evaluate(nbPredictions)\n",
    "print(nbAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "FinalProject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
