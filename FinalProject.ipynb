{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First obtain dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "Y4CJ2heZZHA2"
   },
   "outputs": [],
   "source": [
    "# Pyspark SQL\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pyspark\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF, RegexTokenizer, StopWordsRemover, QuantileDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): langdetect in ./anaconda3/lib/python3.5/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): six in ./anaconda3/lib/python3.5/site-packages (from langdetect)\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package (langdetect) in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the following data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Login information\n",
    "# username = # AWS Username\n",
    "# password = # AWS Password\n",
    "# region = \"us-east-1\" # Change if different from your AWS region\n",
    "\n",
    "\n",
    "# Dataset location\n",
    "# s3 = #s3a address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the AWS resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CluCfBnlZHA7"
   },
   "outputs": [],
   "source": [
    "sc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsAccessKeyId\", username)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsSecretAccessKey\", password)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.\" + region + \".amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = df.rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# small = sc.parallelize(rdd.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes non-English data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def remove_nonenglish(row):\n",
    "    '''\n",
    "    Removes records that have invalid descriptions from the dataframe\n",
    "    Input: dataframe\n",
    "    Output: Cleaned up dataframe\n",
    "    '''\n",
    "    try:\n",
    "        lang=detect(row[1])\n",
    "        if (lang == 'en'): \n",
    "            return True\n",
    "        else: \n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def replace_punc_with_space(desc):\n",
    "    \n",
    "    upd_desc=''\n",
    "    \n",
    "    for i in range(len(desc)-1):\n",
    "        upd_desc+=desc[i]\n",
    "        if desc[i].islower() and desc[i+1].isupper():\n",
    "            upd_desc+=' '\n",
    "    \n",
    "    upd_desc+=desc[-1]\n",
    "    return upd_desc \n",
    "\n",
    "def remove_punc(row):\n",
    "    desc = row[1]\n",
    "    \n",
    "    desc=replace_punc_with_space(desc)    \n",
    "    desc=desc.lower() \n",
    "    desc = \"\".join([\" \" if char in ['.', ',', '?', '!', '(', ')', '/', ';', ':'] else char for char in desc])\n",
    "    desc = \"\".join([\"\" if char in ['\\''] else char for char in desc])\n",
    "    \n",
    "    lst = list(row)\n",
    "    lst[1] = desc\n",
    "    tup = tuple(lst)\n",
    "\n",
    "    return tup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# small = small.filter(remove_nonenglish).map(remove_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genres converted to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genre_to_array(row):\n",
    "    genres = row[10]\n",
    "    glist = []\n",
    "    \n",
    "    if(genres is not None): glist = genres.split('|')\n",
    "    \n",
    "    lst = list(row)\n",
    "    lst[10] = glist\n",
    "    tup = tuple(lst)\n",
    "\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies the above processes to rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = rdd.filter(remove_nonenglish).map(remove_punc).map(genre_to_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Suzanne Collins', 'winning will make you famous  losing means certain death the nation of panem  formed from a post-apocalyptic north america  is a country that consists of a wealthy capitol region surrounded by 12 poorer districts  early in its history  a rebellion led by a 13th district against the capitol resulted in its destruction and the creation of an annual televised event known as the hunger games  in punishment  and as a reminder of the power and grace of the capitol  each district must yield one boy and one girl between the ages of 12 and 18 through a lottery system to participate in the games  the tributes are chosen during the annual reaping and are forced to fight to the death  leaving only one survivor to claim victory when 16-year-old katnisss young sister  prim  is selected as district 12s female representative  katniss volunteers to take her place  she and her male counterpart peeta  are pitted against bigger  stronger representatives  some of whom have trained for this their whole lives    she sees it as a death sentence  but katniss has been close to death before  for her  survival is second nature ', None, 'Hardcover', '9.78044E+12', '374 pages', '4.33', '5519135', '160706', 'The Hunger Games', ['Young Adult', 'Fiction', 'Science Fiction', 'Dystopia', 'Fantasy', 'Science Fiction'], 'https://images.gr-assets.com/books/1447303603l/2767052.jpg'), ('J.K. Rowling|Mary GrandPré', 'there is a door at the end of a silent corridor  and it’s haunting harry pottter’s dreams  why else would he be waking in the middle of the night  screaming in terror harry has a lot on his mind for this  his fifth year at hogwarts  a defense against the dark arts teacher with a personality like poisoned honey  a big surprise on the gryffindor quidditch team  and the looming terror of the ordinary wizarding level exams  but all these things pale next to the growing threat of he-who-must-not-be-named---a threat that neither the magical government nor the authorities at hogwarts can stop as the grasp of darkness tightens  harry must discover the true depth and strength of his friends  the importance of boundless loyalty  and the shocking price of unbearable sacrifice his fate depends on them alll  back cover ', 'US Edition', 'Paperback', '9.78044E+12', '870 pages', '4.48', '2041594', '33264', 'Harry Potter and the Order of the Phoenix', ['Fantasy', 'Young Adult', 'Fiction'], 'https://images.gr-assets.com/books/1255614970l/2.jpg'), ('Harper Lee', 'the unforgettable novel of a childhood in a sleepy southern town and the crisis of conscience that rocked it  to kill a mockingbird became both an instant bestseller and a critical success when it was first published in 1960  it went on to win the pulitzer prize in 1961 and was later made into an academy award-winning film  also a classic compassionate  dramatic  and deeply moving  to kill a mockingbird takes readers to the roots of human behavior - to innocence and experience  kindness and cruelty  love and hatred  humor and pathos  now with over 18 million copies in print and translated into forty languages  this regional story by a young alabama woman claims universal appeal  harper lee always considered her book to be a simple love story  today it is regarded as a masterpiece of american literature ', '50th Anniversary', 'Paperback', '9.78006E+12', '324 pages', '4.27', '3745197', '79450', 'To Kill a Mockingbird', ['Classics', 'Fiction', 'Historical', 'Historical Fiction', 'Academic', 'School'], 'https://images.gr-assets.com/books/1361975680l/2657.jpg')]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data to dataframe with header names and cast datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- rating: decimal(3,2) (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_df = rdd.toDF(['author', 'description', 'edition', 'format', 'isbn13', 'pages', 'rating', 'rating_count', 'review_count', 'title', 'genres', 'image_url']) \\\n",
    "    .drop(\"edition\") \\\n",
    "    .drop(\"format\") \\\n",
    "    .drop(\"pages\") \\\n",
    "    .drop(\"isbn13\") \\\n",
    "    .drop(\"review_count\") \\\n",
    "    .drop(\"image_url\") \\\n",
    "    .drop(\"rating_count\")\n",
    "\n",
    "books_df = books_df.withColumn(\"rating\", books_df[\"rating\"].cast(\"decimal(3,2)\"))\n",
    "books_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "books_df = books_df.dropna(subset=('description', 'rating'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.QuantileDiscretizer\n",
    "\n",
    "Current method is with **QuantileDiscretizer**, which buckets by frequency (equal number in each bucket).\n",
    "\n",
    "Adjust **numBuckets** to change the bucket sizes for the ratings and get different outcomes. The lower number of buckets, the more book ratings per bucket.\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.Bucketizer\n",
    "\n",
    "Alternative is to use **Bucketizer**, which buckets by length (or range). Adjust the numbers in the range to fix the outcome.\n",
    "\n",
    "Sample code:\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "split = [0,0.25,0.5,1,1.25,1.5,1.75,2,2.25,2.5,2.75,3,3.25,3.5,3.75,4,4.25,4.5,4.75,5]\n",
    "bucketizer = Bucketizer(splits=split, inputCol='rating', outputCol='label')\n",
    "books_df = bucketizer.transform(books_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discretizer = QuantileDiscretizer(numBuckets=10, inputCol='rating', outputCol='label')\n",
    "books_df = discretizer.fit(books_df).transform(books_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYdGn8tsZHBQ"
   },
   "source": [
    "### TF/IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources to learn more:\n",
    "\n",
    "* https://spark.apache.org/docs/latest/ml-features#tf-idf\n",
    "* https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "* https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
    "* https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf#scikit-learn-settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- rating: decimal(3,2) (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         description|           descToken|                desc|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|winning will make...|[winning, will, m...|[winning, make, f...|\n",
      "|there is a door a...|[there, is, a, do...|[door, end, silen...|\n",
      "|the unforgettable...|[the, unforgettab...|[unforgettable, n...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer(inputCol=\"description\", outputCol=\"descToken\", pattern=\"\\\\W\")\n",
    "books_df = tokenizer.transform(books_df)\n",
    "\n",
    "swremover = StopWordsRemover(inputCol=\"descToken\", outputCol=\"desc\")\n",
    "books_df = swremover.transform(books_df)\n",
    "\n",
    "books_df[(\"description\", \"descToken\", \"desc\")].show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF\n",
    "\n",
    "**numFeatures should be adjusted to get better outcomes**\n",
    "\n",
    "> Since a simple modulo is used to transform the hash function to a column index, it is advisable to use a power of two as the numFeatures parameter; otherwise the features will not be mapped evenly to the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"desc\", outputCol=\"raw_features\", numFeatures=32)\n",
    "featurized_data = hashingTF.transform(books_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idf_model = idf.fit(featurized_data)\n",
    "rescaled_data = idf_model.transform(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- rating: decimal(3,2) (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- descToken: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- desc: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaled_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+--------------------+\n",
      "|               title|rating|label|            features|\n",
      "+--------------------+------+-----+--------------------+\n",
      "|    The Hunger Games|  4.33|  8.0|(30,[0,1,2,3,4,5,...|\n",
      "|Harry Potter and ...|  4.48|  9.0|(30,[0,1,2,3,4,5,...|\n",
      "|To Kill a Mocking...|  4.27|  8.0|(30,[0,1,3,4,5,6,...|\n",
      "|            Twilight|  3.58|  0.0|(30,[0,3,5,7,9,10...|\n",
      "|      The Book Thief|  4.36|  8.0|(30,[0,1,2,3,4,5,...|\n",
      "|The Chronicles of...|  4.25|  7.0|(30,[0,2,3,4,5,6,...|\n",
      "|  Gone with the Wind|  4.29|  8.0|(30,[0,1,2,3,5,6,...|\n",
      "|The Fault in Our ...|  4.24|  7.0|(30,[0,1,3,4,5,6,...|\n",
      "|   Wuthering Heights|  3.84|  2.0|(30,[0,1,2,3,4,6,...|\n",
      "|   The Da Vinci Code|  3.81|  2.0|(30,[0,1,2,3,4,5,...|\n",
      "+--------------------+------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaled_data.select(\"title\", \"rating\", \"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = rescaled_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes(labelCol=\"label\", featuresCol=\"features\", modelType=\"multinomial\",  smoothing=1.0)\n",
    "nbModel = nb.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbPredictions = nbModel.transform(testData)\n",
    "nbPredictions.select(\"prediction\", \"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "dtModel = dt.fit(trainingData)\n",
    "dtPredictions = dtModel.transform(testData)\n",
    "dtPredictions.select(\"prediction\", \"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtAccuracy = evaluator.evaluate(dtPredictions)\n",
    "print(dtAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "FinalProject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
